---
title: "Time Window Analysis"
author: "windowing team"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation

(Peelle and Van Engen (2020))[https://psyarxiv.com/pc3da/] style multiverse analysis considering possible time windows with logistic growth curve models in a dataset with words of varying frequency, stimuli with varying levels of noise, and with young or old adults. 

```{r load-data}
# get local copy by running peekbank_explore.Rmd
load("data/aoi_data_joined.Rds") 
# change t_range upper limit to 4000 for full window analysis

library(tidyverse)
library(lme4)
library(lmerTest)
library(tictoc)
library(langcog)
library(here)
library(broom.mixed) # offers tidy() for lmer
library(doParallel)
library(foreach)

RUN_MODELS = FALSE
```

For our analysis, we will restrict ourselves to familiar words, and will model age effects.

```{r}
df <- aoi_data_joined %>%
  filter(stimulus_novelty=="familiar", # just familiar, or is the goal to distinguish novel vs. familiar?
         !is.na(age), age > 12, age <= 60) 

rm(aoi_data_joined)
```

First run showed that we have a lot of power -- age effect always p<.001 for any start time x window size.
Do we want to do this analysis by age groups? Maybe if we can find age bins with similarly-sized groups, roughly on the scale of a single typical experiment.

```{r}
df_ones <- aoi_data_joined %>%
  filter(stimulus_novelty=="familiar", 
         age >= 12, age < 24) # 590 subjects..sample 100? 
# or split in half to make comparable to twos and threes?

df_twos <- aoi_data_joined %>%
  filter(stimulus_novelty=="familiar", 
         age >= 24, age < 36) # 224 subjects

df_3s <- aoi_data_joined %>%
  filter(stimulus_novelty=="familiar", 
         age >= 36, age < 48) # 266

rm(aoi_data_joined)
rm(df)
```


## Run Window Models


```{r, eval=RUN_MODELS}
# t_norm = [-1000,4000] 

run_grid <- function(df, outfile) {
  window_min <- -500 # X ms before disambiguation
  window_max <- 2000 #

  # Time bins are 25 ms 
  startTimes <- seq(from = window_min, to = window_max, by = 25) 
  # could use min(pb datasets min time) and max(pb datasets max time)
  windowLengths <- seq(from = 300, to = 2000, by = 25)

param.grid <- expand.grid(startTimes, windowLengths)
names(param.grid) = c("startTimes", "windowLengths")
  
  m1out <- foreach(i = 1:nrow(param.grid), .combine='bind_rows',
                .packages = c("lme4","lmerTest","broom.mixed","tidyverse")) %dopar% {
    # by-trial means
    dft <- df %>% filter(t_norm > param.grid$startTimes[i], 
                         t_norm < (param.grid$startTimes[i] + param.grid$windowLengths[i])) %>%
      rename(target_label = english_stimulus_label) %>%
      group_by(administration_id, trial_id, target_label, distractor_id, age) %>%
      summarise(prop_looking = sum(aoi == "target", na.rm = TRUE) / 
                  (sum(aoi=="target", na.rm=TRUE) + sum(aoi=="distractor", na.rm=TRUE)),
                prop_missing = mean(aoi == "missing", na.rm = TRUE)) %>%
      filter(prop_missing < .3) %>%
      ungroup() %>%
      mutate(age_center = scale(age, scale = FALSE))
    
    # what basic model do we want to run? or do we need to go logistic growth curve?
    mod <- lmer(prop_looking ~ age_center + (1| administration_id) + (1 | target_label),
           data = dft)
    # save coefficients per model
    tidy(mod, effects="fixed") %>% 
      mutate(startTime = param.grid$startTimes[i], 
             windowLength = param.grid$windowLengths[i])
  } 
  write.table(mout, file=paste0("data/",outfile,".csv"))
  return(mout)
}

numCores <- detectCores() - 1
cl <- makeCluster(numCores)
registerDoParallel(cores=numCores)

#run_grid(df, "time-window-results")
mout1 <- run_grid(df_ones, "time-window-12-24mos")
mout2 <- run_grid(df_twos, "time-window-24-36mos")
mout3 <- run_grid(df_twos, "time-window-36-48mos")

stopCluster(cl)
```

## Visualization 

```{r load-model-results}
mout <- read.table(file="data/time-window-results.csv")
```


Visualize significance and magnitude of age coefficient as function of start time and window length.


```{r visualize-p-values}
mout %>% filter(term=="age_center") %>%
  ggplot(aes(x = startTime, y = windowLength, fill = p.value)) +
    geom_raster() +
    theme_bw() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
    coord_equal() + 
    scale_fill_gradient(low = "white", high = "gray", limits = c(0, 1)) +
    xlab("Start Time (ms)") +
    ylab("Window Length (ms)") +
    ggtitle("Significance") 
```


```{r visualize-coefs}
mout %>% filter(term=="age_center") %>%
  ggplot(aes(startTime, windowLength, fill = estimate)) +
    geom_raster() + 
    theme_bw() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
    coord_equal() + 
    scale_fill_gradient2(low = "blue", mid = "white", high = "red", limits = c(0, .01)) +
    xlab("Start Time (ms)") +
    ylab("Window Length (ms)") +
    ggtitle("Estimated Age Coefficient") 
```

## Future Directions

Do the window analyses for different age groups: it may be that a longer time window / later start time is needed for younger children (compared to older children). 
But having very different amounts of data in each age bin may be a problem for power -- perhaps we could subsample or resample to balance the amount of data in each considered subset.